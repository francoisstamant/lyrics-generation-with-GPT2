{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output, State\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "device=torch.device('cpu')\n",
    "model = torch.load('model.pt', map_location=device)\n",
    "model_pop = torch.load('model_pop.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:08<00:00,  8.11s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.33s/it]\n"
     ]
    }
   ],
   "source": [
    "#START OF THE ACTUAL APP INTERFACE\n",
    "app = dash.Dash()\n",
    "\n",
    "#Create the layout of the app\n",
    "app.layout = html.Div([\n",
    "    \n",
    " html.H1(children='LYRICS GENERATOR', style={'textAlign': 'center'\n",
    "        }),\n",
    "    \n",
    "#Add dropdown menu for type of music\n",
    "dcc.Dropdown(id='music_type',\n",
    "    options=[\n",
    "        {'label': 'Rock', 'value': 'Rock'},\n",
    "        {'label': 'Pop', 'value': 'Pop'}\n",
    "    ],\n",
    "    value='Rock'),\n",
    "\n",
    "#Add place where text would be inserted    \n",
    "  dcc.Textarea(\n",
    "        id='textarea-state-example',\n",
    "        value='',\n",
    "        style={'width': '100%', 'height': 200, 'position':'absolute', 'top':'130px'},\n",
    "    ),\n",
    "    html.Button('Submit', id='textarea-state-example-button', n_clicks=0, \n",
    "               style={'position':'absolute', 'left':'800px', 'top':'350px'}),\n",
    "    \n",
    "    html.Div(id='textarea-state-example-output', style={'whiteSpace': 'pre-line', 'position':'absolute', \n",
    "                        'left':'30px', 'top':'400px', 'font-size': 'large','height': '200px', 'width': '700px'})\n",
    "    \n",
    "])  \n",
    "\n",
    "@app.callback(\n",
    "    Output('textarea-state-example-output', 'children'),\n",
    "    [Input('textarea-state-example-button', 'n_clicks'), Input('music_type', 'value')],\n",
    "    State('textarea-state-example', 'value')\n",
    ")\n",
    "\n",
    "def update_output(n_clicks, value, value2):\n",
    "    \n",
    "    if n_clicks > 0:\n",
    "        if value == 'Rock':\n",
    "            generated = generate(model.to('cpu'), tokenizer, value2, entry_count=1)\n",
    "            \n",
    "            #Clean the output\n",
    "            generated2 = ' '.join(generated)\n",
    "            to_remove = generated2.split('.')[-1]\n",
    "            \n",
    "            my_text = generated2.replace(to_remove,'')\n",
    "        \n",
    "        if value == 'Pop':\n",
    "            generated = generate(model_pop.to('cpu'), tokenizer, value2, entry_count=1)\n",
    "            \n",
    "            #Clean the output\n",
    "            generated2 = ' '.join(generated)\n",
    "            to_remove = generated2.split('.')[-1]\n",
    "            \n",
    "            my_text = generated2.replace(to_remove,'')           \n",
    "        \n",
    "        return my_text\n",
    "\n",
    "\n",
    "app.run_server(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
